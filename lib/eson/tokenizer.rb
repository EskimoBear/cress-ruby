require 'oj'
require 'pry'
require_relative 'token_pass'

module Eson::TokenPass
  
  module Tokenizer

    InvalidLexeme = Class.new(StandardError)
    TokenizationIncomplete = Class.new(StandardError)
    
    LANG = Eson::EsonGrammars.format
    
    JsonSymbol = Struct.new :lexeme, :name
    
    #Convert an eson program into a sequence of eson tokens
    #@param eson_program [String] string provided to Eson#read
    #@return [TokenSeq] A token sequence
    #@raise [TokenizationIncomplete] token sequence does not contain
    #  all the characters in the program
    #@eskimobear.specification
    # Eson token set, ET is a set of the eson terminals
    # Eson token, et is a sequence of characters existing in ET
    # label(et) maps the character sequence to the name of the matching
    #   eson terminal symbol
    # Input program, p, a valid JSON string 
    # Input sequence, P, a sequence of characters in p
    # Token sequence, T
    #
    # Init : length(P) > 0
    #        length(T) = 0
    # Next : et = P - 'P
    #        T' = T + label(et)
    def tokenize_program(eson_program)
      eson_program.freeze
      program_json_hash = Oj.load(eson_program)
      program_char_seq = get_program_char_sequence(program_json_hash)
      json_symbol_seq = get_json_symbol_sequence(program_json_hash)
      token_seq = json_symbols_to_tokens(json_symbol_seq, program_char_seq)
      unless program_char_seq.empty?
        raise TokenizationIncomplete,
              tokenization_incomplete_error_message
      end
      token_seq
    end

    private

    def tokenization_incomplete_error_message
      "The sequence of eson tokens generated by the" \
      " compiler only partially represents the program." \
      " Compilation cannot continue; please file a bug" \
      " report providing the eson program tried."
    end

    def get_program_char_sequence(hash)
      seq = Array.new
      compact_string = Oj.dump(hash)
      compact_string.each_char {|c| seq << c}
      seq
    end

    def get_json_symbol_sequence(hash)
      Array.new.push(JsonSymbol.new(:"{", :object_start))
        .push(members_to_json_symbols(hash))
        .push(JsonSymbol.new(:"}", :object_end))
        .flatten
    end

    def members_to_json_symbols(json_pairs)
      seq = Array.new
      unless json_pairs.empty?
        seq.push pair_to_json_symbols(json_pairs.first)
        rest = json_pairs.drop(1)
        unless rest.empty?
          rest.each_with_object(seq) do |i, seq|
            seq.push(JsonSymbol.new(:",", :member_comma))
              .push(pair_to_json_symbols(i))
          end
        end
      end
      seq
    end

    def pair_to_json_symbols(json_pair)
      json_value = json_pair[1]
      value = value_to_json_symbols(json_value)
      Array.new.push(JsonSymbol.new(json_pair.first, :JSON_key))
        .push(JsonSymbol.new(:":", :colon))
        .push(value)
        .flatten
    end

    def value_to_json_symbols(json_value)
      if json_value.is_a? Hash
        get_json_symbol_sequence(json_value)
      elsif json_value.is_a? Array
        array_to_json_symbols(json_value)
      else
        JsonSymbol.new(json_value, :JSON_value)
      end
    end

    def array_to_json_symbols(json_array)
      seq = Array.new.push(JsonSymbol.new(:"[", :array_start))
      unless json_array.empty?
        seq.push(value_to_json_symbols(json_array.first))
        unless json_array.drop(1).empty?
          json_array.drop(1).each do |i|
            seq.push(JsonSymbol.new(:",", :array_comma))
            seq.push(value_to_json_symbols(i))
          end
        end
      end
      seq.push(JsonSymbol.new(:"]", :array_end))                     
    end

    def json_symbols_to_tokens(json_symbol_seq, char_seq)
      envs = [{:attr => :line_no, :attr_value => 1},
              {:attr => :indent, :attr_value => 0},
              {:attr => :spaces_after, :attr_value => 1}]
      json_symbol_seq.each_with_object(Eson::TokenPass::TokenSeq.new) do |symbol, seq|
        case symbol.name
        when :object_start
          update_json_and_char_seqs(
            LANG.program_start.make_token(symbol.lexeme, envs),
            seq,
            char_seq,
            envs)
        when :object_end
          update_json_and_char_seqs(
            LANG.program_end.make_token(symbol.lexeme, envs),
            seq,
            char_seq,
            envs)
        when :array_start
          update_json_and_char_seqs(
            LANG.array_start.make_token(symbol.lexeme, envs),
            seq,
            char_seq,
            envs)
        when :array_end
          update_json_and_char_seqs(
            LANG.array_end.make_token(symbol.lexeme, envs),
            seq,
            char_seq,
            envs)
        when :colon
          update_json_and_char_seqs(
            LANG.colon.make_token(symbol.lexeme, envs),
            seq,
            char_seq,
            envs)
        when :array_comma
          update_json_and_char_seqs(
            LANG.comma.make_token(symbol.lexeme, envs),
            seq,
            char_seq,
            envs)
        when :member_comma
          update_json_and_char_seqs(
            LANG.declaration_divider.make_token(symbol.lexeme, envs),
            seq,
            char_seq,
            envs)
        when :JSON_key
          tokenize_json_key(symbol.lexeme, seq, char_seq, envs)
        when :JSON_value
          tokenize_json_value(symbol.lexeme, seq, char_seq, envs)
        end
      end
    end

    #Accumulator function for sequences and environment variables
    #used in the tokenization process.
    #@param token [Token]
    #@param token_seq [TokenSeq]
    #@param char_seq [Array]
    def update_json_and_char_seqs(token, token_seq, char_seq, envs)
      token_seq.push(token)
      char_seq.slice!(0, token.lexeme.size)
      update_line_no_env(envs, token, token_seq)
      update_indent_env(envs, token, token_seq)
    end

    def update_line_no_env(envs, token, token_seq)
      end_line_tokens = [:program_start,
                         :array_start,
                         :comma,
                         :declaration_divider]
      start_line_tokens = [:program_end,
                           :array_end]
      if end_line_tokens.include?(token.name)
        inc_line_no(envs)
      elsif start_line_tokens.include?(token_seq.last.name)
        inc_line_no(envs)
        token_seq.last.eval_s_attributes(envs)
      end
    end

    def update_indent_env(envs, token, token_seq)
      end_line_tokens = [:program_start,
                         :array_start]
      start_line_tokens = [:program_end,
                           :array_end]
      if end_line_tokens.include?(token.name)
        inc_indent(envs)
      elsif start_line_tokens.include?(token_seq.last.name)
        dec_indent(envs)
        token_seq.last.eval_s_attributes(envs)
      end
    end

    def inc_line_no(envs)
      env = envs.find{|i| i[:attr] == :line_no}
      env[:attr_value] = env[:attr_value] + 1
    end

    def inc_indent(envs)
      env = envs.find{|i| i[:attr] == :indent}
      env[:attr_value] = env[:attr_value] + 1
    end

    def dec_indent(envs)
      env = envs.find{|i| i[:attr] == :indent}
      env[:attr_value] = env[:attr_value] - 1
    end

    def tokenize_json_key(json_key, seq, char_seq, envs)
      lexer([:special_form_identifier,
             :unreserved_procedure_identifier,
             :attribute_name],
            get_delimited_string(json_key),
            seq,
            char_seq,
            envs)
    end

    def get_delimited_string(string)
      "\"".concat(string).concat("\"")
    end

    def lexer(terminals, string, seq, char_seq, envs)
      matched_terminal = terminals.detect{|i| LANG.send(i).match(string)}
      if matched_terminal.nil?
        raise InvalidLexeme, lexer_error_message(string)
      else
        update_json_and_char_seqs(
          LANG.send(matched_terminal).match_token(string, envs),
          seq,
          char_seq,
          envs)
      end
    end

    def lexer_error_message(string)
      "The string - \n\"#{string}\"\ncould not be broken up into tokens." \
      " It does not match any of the valid tokens in eson."
    end

    def tokenize_json_value(json_value, seq, char_seq, envs)
      if json_value.is_a? TrueClass
        update_json_and_char_seqs(
          LANG.true.make_token(json_value.to_s, envs),
          seq,
          char_seq,
          envs)
      elsif json_value.is_a? FalseClass
        update_json_and_char_seqs(
          LANG.false.make_token(json_value.to_s, envs),
          seq,
          char_seq,
          envs)
      elsif json_value.is_a? Numeric
        update_json_and_char_seqs(
          LANG.number.make_token(json_value.to_s, envs),
          seq,
          char_seq,
          envs)
      elsif json_value.nil?
        update_json_and_char_seqs(
          LANG.null.make_token(:null, envs),
          seq,
          char_seq,
          envs)
      elsif json_value.is_a? String
        tokenize_json_string(
          get_delimited_string(json_value),
          seq,
          char_seq,
          envs)
      end
    end

    def tokenize_json_string(json_string, seq, char_seq, envs)
      lexer(
        [:string_delimiter,
         :word_form,
         :variable_identifier],
        json_string,
        seq,
        char_seq,
        envs)
      rest = get_rest(json_string, seq)
      unless rest.empty?
        tokenize_json_string(rest, seq, char_seq, envs)
      end
    end

    def get_rest(string, seq)
      matched_string = seq.last.lexeme
      string[matched_string.size..-1]
    end
  end
end
